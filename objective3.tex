
\msection{Attentional Filtering (Objective 3) }
\label{sec:aim3}



\biobackground{}
Models of attention are central understanding cognition in the brain.
At higher levels of processing we actively select which
dimensions to process based on their inherent salience and relevance
to our goals. Consider the task of reconstructing a person's
subjective experience of a movie from their brain data. Existing
inverse modeling methods attempt to reproduce the pixels of the photos
or movie frames being shown to the person, but if the human brain only
represents a subset of this information these models have an inherent
performance ceiling. Pixels are not ground truth for the brain, but
the problem is that there is no principled way to figure out the
internal ``ground truth''  (which can also vary from moment to moment). A
useful public dataset for examining this is 17 subjects who watched a
90 minute episode of Sherlock while being scanned with fMRI. We have a
frame-by-frame annotation of several dimensions of the movie, and so
could try to reconstruct which of these dimensions are present in the
brain data and how this attentional trace changes dynamically over
time.


\statbackground{} High-dimensional problems suffer from the curse of
dimensionality. This has a precise mathematical characterization in
standard models of statistical machine learning. The curse of
dimensionality has two components, one statistical, the other
computational. The statistical issue stems from the observation that
in high dimensions, any local ball will contain very few data
points. The computational curse is implied by the fact that searching
over a sufficiently large space of models is often
intractable. ``Beating'' the curse of dimensionality involves making
assumptions about the structure of the learning problem. We will study
mathematical formulations of attention as one approach to making
assumptions for which learning is tractable in principle.
In an attention-based model, the object of study is a
lower-dimensional trace or curve through the high-dimensional input
space. 

In the deep learning literature, attention-based models were first
developed in the setting of machine translation, using
sequence-to-sequence algorithms based on recurrent neural networks
(RNNs) \citep{bahdanau2014}. The attention mechanism is a type of alignment model,
which is a key component of statistical translation methods
\citep{Brown1993}. Attention has been applied to the problem of generating image
descriptions by \cite{showtell}.

Recall from Section~\ref{sec:aim1} that an exponential family
embedding model uses a latent representation of the input and output
variables. Consider the case of high dimensional density estimation,
where the goal is to estimate a density $p(x)$ for $x\in\reals^d$.
A latent variable embedding model takes the form
$$ p(x) = \int p(z) \exp(\rho(x)^T \alpha(z) - \Psi_{\rho,\alpha}) \, dz$$
where $z\in \reals^m$ is a latent Gaussian vector,
$\rho:\reals^d \to \reals^K$ is an embedding of the input space,
and $\alpha: \reals^m \to\reals^K$ is an embedding of the latent
Gaussian, and $\Psi_{\rho,\alpha}$ is a normalizing constant.
We will investigate attention-based versions of embedding
models. Many possibilities exist for doing so. For instance, consider a parameterized curve $t\mapsto (Z_t, S_t)$
where $Z_t$ is a Gaussian vector and $S_t \subset \{1,\ldots, d\}$
is a subset of the input variables. Define the inner product 
$\langle x, z\rangle_{\rho,\alpha}
= \int_{0}^1 \rho\left(x_{S(t)}\right)^T \alpha(Z_t) dt,$
and the corresponding embedding model
$ p(x) = \int p(z) \exp\left(\langle x, z\rangle_{\rho,\alpha} - \Psi_{\rho,\alpha}\right) dz.$
In another version, consider a graph over $\{1,\ldots, d\}$ with edge
set $E$, and define the inner product
$\langle x, z\rangle_{\rho,\alpha}
= \sum_{(j,k)\in E} \rho(x_j, x_k)^T \alpha(Z_j, Z_k)$
where $Z$ is now a vector-valued Gaussian random field, 
and the model can be viewed as a nonparametric graphical model
\citep{hl18}.

These are thought of as attention-based models as the ``energy''
$\langle x, z\rangle_{\rho,\alpha}$ is localized to subsets of the
sample space. Discriminative or regression-based versions of these
models are defined similarly. The project will develop variational
inference algorithms for this family of models. When the mapping 
$\alpha(z)$ is defined in terms of, for instance, a feed-forward
neural network, this becomes an instance of a variational auto-encoder
\citep{kingma13}.


\vskip20pt

\project{Algorithms using variational inference}

\project{Learning complexity under attention}

\project{Applications to fMRI movie data}

\project{Experiments with fMRI to test attention models}



 
