\let\tilde\widetilde
\def\SS{{\mathbb S}}
\def\X{{\mathbb X}}
%\def\bb{\mbox{\boldmath$b$}}
\def\bb{b}
\def\noisesd{\sigma}
\def\C{{\mathcal C}}
\def\A{{\mathcal A}}
\def\bigbracket#1#2#3{
\left[ 
\mbox{\begin{minipage}{#1}{
\vskip#2
#3
\vskip#2
\mbox{\ }
}\end{minipage}}
\right]
}

\msection{Data Representation (Objective 2)}
\label{sec:aim2}

The brain persistently stores the same information in several
different ways across regions, each emphasizing different dimensions
of the input. In doing so, different features become readily
accessible to the computations implemented in these regions. More
generally, by representing information in different ways, similar
learning rules are able to automatically aggregate experience along
different dimensions and thereby extract different and complementary
knowledge. This can be contrasted with machine learning, both by the
simple fact that data are stored on disk in one format and that
finding relevant dimensions is often the output of the learning
algorithm rather a natural consequence of how the input is structured.
We will develop algorithms for machine representation of data that are
informed by our understanding of how the brain represents information
in different systems and leverage developments in embedding algorithms
in machine learning for advanced processing of fMRI and cellular
recordings and the information they contain.

\biobackground{} Multiple brain systems represent inputs from the
world and store this information in different memory registers.
Although these representations are sometimes redundant (e.g., across
hemispheres in the same brain system), they often emphasize different
aspects of the input that are extracted via different neural pathways
or as a result of transformations across brain regions. In visual
cortex, for example, different inputs with similar appearance (e.g.,
the faces of siblings, the Grand Canyon from different vistas, bananas
in a grocery store, etc.) will be stored together. In frontotemporal
cortex, different inputs with the same conceptual, semantic, or
functional meaning will be stored together despite differences in
appearance (e.g., pieces of clothing, cooking utensils, animals in the
ocean, etc.). In the ventral striatum and orbitofrontal cortex,
different inputs with the same reward value will be represented
similarly irrespective of appearance or meaning (although subjective,
e.g., a favorite t-shirt and a nostalgic meal, an art show and a music
performance). Finally, in the hippocampus, overlap in appearance,
meaning, and reward can be discounted in favor of representing inputs
that co-occur over space or time together (e.g., a sequence of
landmarks on a commute, the people in a social group, events on a
memorable date, etc.). There are other dimensions that organize or
dominate the representations in other brain systems, such as emotion,
modality, tasks, and motor actions. However, here we will focus on
visual cortex and the hippocampus, two brain systems with mature
theories that address the nature of their representations.

After passing from the retina and through subcortical structures, the
human visual system is a hierarchical set of cortical brain areas
starting from the first visual area (V1) in the calcarine sulcus of
the occipital lobe~\citep{Felleman:1991}. There are four primary
organizing principles from there. First, the visual areas in each
hemisphere receive input from the space contralateral to where the
eyes are fixated (i.e., retinotopic), with the input from a given side
of space projecting to both eyes which converge in ocular dominance
columns in V1. Second, the areas are arrayed into two
streams~\citep{Mishkin:1983,Goodale:1992}: the ventral stream passing
on the inferior surface into the temporal lobe with areas coding for
the contralateral upper quadrant (V1-V3) and then the contralateral
hemifield (V4 and beyond) involved in recognizing the identity of
objects~\citep{Arcaro:2009}; the dorsal stream passing on the superior
surface into the parietal then frontal lobes with areas coding for the
contralateral lower quadrant (V1-V3) and hemifield (V3a and beyond)
involved in processing information about object location and
action~\citep{Konen:2008}. Third, the visual areas in each stream
represent stimulus dimensions of increasing complexity: for example,
from edges in V1, to color and texture in V4, to shape in ventral
occipital, to object identity and category in inferior temporal (IT)
cortex~\citep{Grill-Spector:2003,Rousselet:2004}. Finally, paired with
this increasing complexity is greater flexibility or ``tolerance'' to
variation in more basic features, both in terms of neurons in higher
areas having larger areas of space (i.e., receptive fields) over which
they respond and in terms of invariance of neural representations, in
IT for example, to size, position, viewpoint, etc.~\citep{Rust:2010}.
The properties parallel those of deep convolutional neural networks
for object and scene recognition~\citep{Kriegeskorte:2015}, and in
fact optimizing the architecture of a deep net for object
recognition, without consideration of brain data, naturally
results in a model that predicts neuronal responses in visual
cortex~\citep{Yamins:2016}. The visual system is thus one of the best
understood neurobiological systems and also the one that has had the
greatest impact to date on machine learning.

Most models of ventral visual processing treat IT as the top of
the hierarchy. However, IT is heavily interconnected with
the next most anterior areas, perirhinal cortex (PRC) and
parahippocampal cortex (PHC), which show selectivity for
objects and scenes,
respectively~\citep{Barense:2010,Davachi:2006,Ranganath:2012}. The PRC
and PHC project to the entorhinal cortex (ERC). The ERC in turn
provides input to the hippocampus, which is critical for storing
long-term memories~\citep{Squire:1992}. Within the hippocampus, there
are four key subfields: dentate gyrus (DG), CA3, CA1, and
subiculum~\citep{Deng:2010,Shohamy:2013}. They are
connected to form two pathways: the trisynaptic pathway (TSP) from ERC
to DG, CA3, and CA1; and the monosynaptic pathway (MSP) from ERC to
CA1~\citep{Schapiro:2017}. The hippocampus also has two sources of
recurrence that allow it to bridge over time: locally within CA3 and
as a result of CA1 output to ERC that re-enters as
input~\citep{Kumaran:2016}. The function of the TSP circuit is to
memorize specific input patterns from ERC, supporting ``episodic''
memory for individual moments in space and time (e.g., where I parked
my car this morning). This poses two computational challenges: the
patterns need to be learned ``one-shot'' in that we never experience
the exact same episode twice, and it is critical to avoid interference
from related memories (e.g., where I parked by car yesterday). The TSP
solves these challenges with \textit{pattern
separation}~\citep{Leutgeb:2007,Yassa:2011,Rolls:2016}: large capacity,
strong inhibition, and partial connectivity in DG and then CA3 lead to
sparse, orthogonal representations of even highly overlapping ERC
input patterns that share sensory features (e.g., tend to park in
the same neighborhood, at the same time of day, in the same vehicle,
etc.). The resulting memories are complex, unfurling over space and
time with input from different modalities, and these different
components are linked via the auto-associative connections in
CA3~\citep{Wallenstein:1998}. By discounting feature overlap across
episodes and binding even arbitrary features within episodes, such
spatiotemporal co-occurrence dominates hippocampal representation, allowing for recall of individual experiences.
Whereas theoretical models of the hippocampus from the connectionist
tradition have been around a long time and employ multi-layer
networks~\citep{McClelland:1995,Norman:2003}, the computational
properties of the hippocampus (e.g., pattern separation, different
learning rates, bridging over time, relational binding, etc.) have not
yet entered the mainstream of machine learning.

\setlength{\columnsep}{20pt}
\begin{wrapfigure}{R}{0.45\textwidth}
\centering
\includegraphics[width=.44\textwidth]{figs/hipp-model}
\caption{\small Biologically plausible neural network model of the
hippocampus. The network is trained to reproduce the pattern of
activity in the input (EC\textsubscript{in}) on the output
(EC\textsubscript{out}). Three hidden layers (DG, CA3, and CA1) learn
representations to support this mapping, with activity flow governed
by projections indicated by the arrows. The TSP, in blue
arrows, supports encoding into episodic memory via pattern separation
in DG and CA3 and retrieval via pattern completion within CA3. The
MSP, in green arrows, supports statistical learning of
regularities across episodes. The height/yellowness of a unit both
index its activity level. From ~\citet{Schapiro:2017}.}
    \label{fig:hipp}
    \vskip2pt
\end{wrapfigure}

\statbackground{}
Data representation algorithms are an important ingredient in contemporary
machine learning methodology. A particular type, called embeddings, are used to represent
categorical data---for example words in a natural language text---as
dense, high dimensional ($d\approx 500$) vectors in Euclidean space \citep{Mikolov:2013}.
Embedding algorithms give representations that are well suited for downstream
algorithms. Sometimes they are learned in the first layer of
a deep neural network ~\citep{Bengio:2003}.  

Embeddings can be useful for many data types. For example, the popular
music entertainment site Spotify constructs embeddings of songs from
millions of user playlists, and uses these as the basis for
recommending new songs.  (``If you liked song $x\in\reals^{500}$,
perhaps you'll like the nearby song $x'\in\reals^{500}$.'')  This
machine learning based system eclipsed Pandora, which was based on a
hand-constructed representation, the ``music genome.'' Yet current
embedding algorithms such as the popular \texttt{word2vec} algorithm
are based on little more than a low-rank approximation (PCA) of simple
co-occurrence statistics.

Embeddings algorithms become more flexible when they are based
on statistical models, for instance using exponential family
models \citep{Rudolph:2016b,spherical}.
Consider a corpus of language $\mbx = \{x_1, \ldots, x_n\}$, where
each $x_i$ is a word from a vocabulary of terms. An exponential family
embedding has three components: (a) a notion of context for
each data point, e.g., a window of observed words around each word (b)
a form of the conditional distribution, e.g., for text a
categorical distribution over $V$ items is appropriate and (c) an
embedding structure that describes how parameters are shared
across data, e.g., for text one may assume that each term (such as
``walnut'' or ``bicycle'') shares the same representations wherever it
appears in the collection.
An exponential family embedding
posits two $d$-dimensional latent representations for each term $v$,
one is the embedding vector $\rho_v$ and the other is the
context vector $\alpha_v$, where $d$ is a hyperparameter.  The
model asserts that each observation is drawn from a conditional
distribution given its context. 
Exponential family embeddings generalize many existing methods for
learning distributed representations, including 
many variants of word2vec~\citep{Mikolov:2013}.  


\setlength{\columnsep}{20pt}
\begin{wrapfigure}{R}{0.45\textwidth}
\centering
\includegraphics[width=.44\textwidth]{figs/ginsparg}
\caption{\small Embeddings of words appearing in scientific articles
posted on the arXiv, mapped from 200 to two dimensions using
t-SNE \citep{ginsparg}. Constructed using only three months of
arXiv articles, for a 130,000 word vocabulary---using more data
leads to better embeddings.
Interactive exploration of the embeddings
using a Google maps interface: \href{http://www.cs.cornell.edu/~ginsparg/arxiv/gmaps2.html}{cs.cornell.edu/~ginsparg/arxiv/gmaps2.html}.
}
    \label{fig:arxiv}
    \vskip2pt
\end{wrapfigure}

Fitting such models is difficult, and requires robust methods for
computation and evaluation.  Variational inference approximates the
posterior by fitting a family of distributions over the latent
variables to be close in Kullback-Leibler
divergence~\citep{Jordan:1999,Blei:2017}.  In the case of embeddings,
the variational distribution is $q(\alpha_{1:V}, \rho_{1:V} ; \nu)$
and we fit the variational parameters $\nu$ to be close in KL
divergence to $p(\alpha_{1:V}, \rho_{1:V} \g \mbx)$.  Recent
innovations in black box variational inference~\citep{Ranganath:2014},
probabilistic programming~\citep{Kucukelbir:2017,Tran:2017}, and
variational autoencoders
\citep{kingma} as implemented, for example, in Google TensorFlow, allow us to do this generically and
scalably, easily fitting many types of models to large
data sets.  This enables the exploration of many variants of the
models, e.g., different types of contexts, different values of $d$,
and different underlying conditional distributions. We will use
this framework to develop more advanced and useful embedding
algorithms, inspired by current knowledge of higher-level cognition.


\project{Representations beyond co-occurrence statistics}
Distributed embedding representations in machine learning are almost
exclusively based on co-occurrence statistics. For instance, when
constructing embeddings for words in text, names of colors (``red,''
``blue''...) will be embedded in nearby locations simply because they
tend to be used together. How can a richer knowledge of representation
in the brain be used to inspire algorithms for processing text,
images, and audio? As discussed above, the brain systems roughly
``embed'' as follows: visual cortex/appearance, frontotemporal
cortex/function, ventral striatum/reward, hippocampus/co-occurrence.
We will explore a wide variety of new machine learning models 
for representing data, using brain function as inspiration
for coding different aspects of data. For instance,
one embedding component learned in a reinforcement learning
setting might correspond to processing in the orbitofrontal cortext;
another embedding component based on semantic parsing of 
languages or scenes might correspond to processing in
the frontotemporal cortex. 
%Steps in such directions, have
%rguably made in recent work \citep{replearn}. 
The framework of exponential family embeddings and variational
inference provide powerful tools for this investigation.


\project{Embeddings for fMRI data}
In the other direction, from machine learning to cognitive
neuroscience, we believe that latent variable models and exponential family embeddings are
promising tools for advanced data analysis for fMRI.
[{\it Nick -- if you want to write a few sentences about the methodologies
that dominate fMRI analysis now, I can then try to comment on
how new machine learning methodologies might be applied.}]


\project{Time-dependent, multiscale representations}
As discussed above, known computational properties of the hippocampus
including pattern separation, different learning rates, and bridging
over time, have not yet entered the mainstream of machine learning.
We will study new learning architectures that attempt to mimic, even
if only as a computational metaphor, these capabilities in cognition.
To capture dependence on time, we will earlier work on dynamic topic
models~\citep{Blei:2006d}.  Dynamic topic models capture how the
latent themes in a collection can grow and shrink and change over
time.  By introducing multiple latent time series threads in such a
model, each having a different time scale, we can model different
learning rates in a dynamic model.  Dynamic topic models were
developed specifically for language.  We will generalize this idea to
capture the evolution over time of distributed representations.  In
the exponential family embedding framework, this amounts to placing a
linear dynamic prior on the embedding vectors or the context vectors,
or both.
