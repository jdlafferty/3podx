\let\tilde\widetilde
\def\SS{{\mathbb S}}
\def\X{{\mathbb X}}
%\def\bb{\mbox{\boldmath$b$}}
\def\bb{b}
\def\noisesd{\sigma}
\def\C{{\mathcal C}}
\def\A{{\mathcal A}}
\def\bigbracket#1#2#3{
\left[ 
\mbox{\begin{minipage}{#1}{
\vskip#2
#3
\vskip#2
\mbox{\ }
}\end{minipage}}
\right]
}

\msection{Data Representation (Objective 2)}
\label{sec:aim2}

The brain persistently stores the same information in several
different ways across regions, each emphasizing different dimensions
of the input. In doing so, different features become readily
accessible to the computations implemented in these regions. More
generally, by representing information in different ways, similar
learning rules are able to automatically aggregate experience along
different dimensions and thereby extract different and complementary
knowledge. This can be contrasted with machine learning, both by the
simple fact that data are stored on disk in one format and that
finding relevant dimensions is often the output of the learning
algorithm rather a natural consequence of how the input is structured.
We will develop algorithms for machine representation of data that are
informed by such understanding of how the brain represents information
in different systems and leverage developments in embedding algorithms
in machine learning for advanced processing of fMRI and cellular
recordings and the information they contain.

\biobackground{} Multiple brain systems represent inputs from the
world and store this information in different memory registers.
Although these representations are sometimes redundant (e.g., across
hemispheres in the same brain system), they often emphasize different
aspects of the input that are extracted via different neural pathways
or as a result of transformations across brain regions. In visual
cortex, for example, different inputs with similar appearance (e.g.,
the faces of siblings, the Grand Canyon from different vistas, bananas
in a grocery store, etc.) will be stored together. In frontotemporal
cortex, different inputs with the same conceptual, semantic, or
functional meaning will be stored together despite differences in
appearance (e.g., pieces of clothing, cooking utensils, animals in the
ocean, etc.). In the ventral striatum and orbitofrontal cortex,
different inputs with the same reward value will be represented
similarly irrespective of appearance or meaning (although subjective,
e.g., a favorite t-shirt and a nostalgic meal, an art show and a music
performance). Finally, in the hippocampus, overlap in appearance,
meaning, and reward can be discounted in favor of representing inputs
that co-occur over space or time together (e.g., a sequence of
landmarks on a commute, the people in a social group, events on a
memorable date, etc.). There are other dimensions that organize or
dominate the representations in other brain systems, such as emotion,
modality, tasks, and motor actions. However, here we will focus on
visual cortex and the hippocampus, two brain systems with mature
theories that address the nature of their representations.

After passing from the retina and through subcortical structures, the
human visual system is a hierarchical set of cortical brain areas
starting from the first visual area (V1) in the calcarine sulcus of
the occipital lobe~\citep{Felleman:1991}. There are four primary
organizing principles from there. First, the visual areas in each
hemisphere receive input from the space contralateral to where the
eyes are fixated (i.e., retinotopic), with the input from a given side
of space projecting to both eyes which converge in ocular dominance
columns in V1. Second, the areas are arrayed into two
streams~\citep{Mishkin:1983,Goodale:1992}: the ventral stream passing
on the inferior surface into the temporal lobe with areas coding for
the contralateral upper quadrant (V1-V3) and then the contralateral
hemifield (V4 and beyond) involved in recognizing the identity of
objects~\citep{Arcaro:2009}; the dorsal stream passing on the superior
surface into the parietal then frontal lobes with areas coding for the
contralateral lower quadrant (V1-V3) and hemifield (V3a and beyond)
involved in processing information about object location and
action~\citep{Konen:2008}. Third, the visual areas in each stream
represent stimulus dimensions of increasing complexity: for example,
from edges in V1, to color and texture in V4, to shape in ventral
occipital, to object identity and category in inferior temporal (IT)
cortex~\citep{Grill-Spector:2003,Rousselet:2004}. Finally, paired with
this increasing complexity is greater flexibility or ''tolerance'' to
variation in more basic features, both in terms of neurons in higher
areas having larger areas of space (i.e., receptive fields) over which
they respond and in terms of invariance of neural representations, in
IT for example, to size, position, viewpoint, etc.~\citep{Rust:2010}.
The properties parallel those of deep convolutional neural networks
for object and scene recognition~\citep{Kriegeskorte:2015}, and in
fact optimizing the architecture of a deep net for object
recognition, without consideration of brain data, naturally
results in a model that predicts neuronal responses in visual
cortex~\citep{Yamins:2016}. The visual system is thus one of the best
understood neurobiological systems and also the one that has had the
greatest impact to date on machine learning.

Most models of ventral visual processing treat IT as the top of
the hierarchy. However, IT is heavily interconnected with
the next most anterior areas, perirhinal cortex (PRC) and
parahippocampal cortex (PHC), which show selectivity for
objects and scenes,
respectively~\citep{Barense:2010,Davachi:2006,Ranganath:2012}. The PRC
and PHC project to the entorhinal cortex (ERC). The ERC in turn
provides input to the hippocampus, which is critical for storing
long-term memories~\citep{Squire:1992}. Within the hippocampus, there
are four key subfields: dentate gyrus (DG), CA3, CA1, and
subiculum~\citep{Deng:2010,Shohamy:2013}. They are
connected to form two pathways: the trisynaptic pathway (TSP) from ERC
to DG, CA3, and CA1; and the monosynaptic pathway (MSP) from ERC to
CA1~\citep{Schapiro:2017}. The hippocampus also has two sources of
recurrence that allow it to bridge over time: locally within CA3 and
as a result of CA1 output to ERC that re-enters as
input~\citep{Kumaran:2016}. The function of the TSP circuit is to
memorize specific input patterns from ERC, supporting ''episodic''
memory for individual moments in space and time (e.g., where I parked
my car this morning). This poses two computational challenges: the
patterns need to be learned ''one-shot'' in that we never experience
the exact same episode twice, and it is critical to avoid interference
from related memories (e.g., where I parked by car yesterday). The TSP
solves these challenges with \textit{pattern
separation}~\citep{Leutgeb:2007,Yassa:2011,Rolls:2016}: large capacity,
strong inhibition, and partial connectivity in DG and then CA3 lead to
sparse, orthogonal representations of even highly overlapping ERC
input patterns that share sensory features (e.g., tend to park in
the same neighborhood, at the same time of day, in the same vehicle,
etc.). The resulting memories are complex, unfurling over space and
time with input from different modalities, and these different
components are linked via the auto-associative connections in
CA3~\citep{Wallenstein:1998}. By discounting feature overlap across
episodes and binding even arbitrary features within episodes, such
spatiotemporal co-occurrence dominates hippocampal representation, allowing for recall of individual experiences.
Whereas theoretical models of the hippocampus from the connectionist
tradition have been around a long time and employ multi-layer
networks~\citep{McClelland:1995,Norman:2003}, the computational
properties of the hippocampus (e.g., pattern separation, different
learning rates, bridging over time, relational binding, etc.) have not
yet entered the mainstream of machine learning.

\statbackground{}
``Embedding'' algorithms are an important ingredient in contemporary
machine learning methodology. An embedding algorithm represents 
categorical data---notably words in a natural language text---as
high dimensional ($d\approx 500$) vectors in Euclidean space. 
This gives a representation that is well suited for downstream
algorithms; sometimes the embeddings are learned in the first layer of
a deep neural network ~\citep{Bengio:2003,Mikolov:2013}.  

Embeddings can be extremely useful. Indeed, the popular music
entertainment site Spotify constructs embeddings of songs from
millions of user playlists, and uses these as the basis for
recommending new songs.  (If you liked song $x\in\reals^{500}$,
perhaps you'll like $x'\in\reals^{500}$, since
$\|x-x'\|\leq \epsilon$.) This machine learning based
system eclipsed Pandora, which was based on a hand-constructed
representation, the ``music genome.'' Yet current embedding
algorithms are based on little more than a low-rank approximation
(PCA) of simple coorrence statistics. (In an undergraduate machine
learning course at Yale, co-PI Lafferty has students prove as a simple
exercise that the popular \texttt{word2vec} algorithm takes this form.)

Embeddings take a more sophisticated and flexible form when they are based
on statistical models. For instance, exponential family embeddings are a new way to
generalize classical methods of finding distributed representations in
language~\citep{Rudolph:2016b}.
Consider a corpus of language $\mbx = \{x_1, \ldots, x_n\}$, where
each $x_i$ is a word from a vocabulary of terms. An exponential family
embedding has three components: (a) a notion of context for
each data point, e.g., a window of observed words around each word (b)
a form of the conditional distribution, e.g., for text a
categorical distribution over $V$ items is appropriate and (c) an
embedding structure that describes how parameters are shared
across data, e.g., for text we typically posit that each term (such as
``walnut'' or ``bicycle'') shares the same representations wherever it
appears in the collection.
An exponential family embedding
posits two $d$-dimensional latent representations for each term $v$,
one is the embedding vector $\rho_v$ and the other is the
context vector $\alpha_v$, where $d$ is a hyperparameter.  The
model asserts that each observation is drawn from a conditional
distribution given its context. 
Exponential family embeddings generalize many existing methods for
learning distributed representations, including 
many variants of word2vec~\citep{Bengio:2003,Mikolov:2013}.  


\setlength{\columnsep}{20pt}
\begin{wrapfigure}{R}{0.45\textwidth}
\centering
\includegraphics[width=.44\textwidth]{figs/ginsparg}
\caption{\small Embeddings of words appearing in scientific articles
posted on the arXiv, mapped from 200 to two dimensions using
t-SNE \citep{ginsparg}. Constructed using only three months of
arXiv articles, for a 130,000 word vocbulary---using more data
leads to better embeddings.
Interactive exploration of the embeddings
using a Google maps interface: \href{http://www.cs.cornell.edu/~ginsparg/arxiv/gmaps2.html}{cs.cornell.edu/~ginsparg/arxiv/gmaps2.html}.
}
    \label{fig:arxiv}
    \vskip2pt
\end{wrapfigure}

Fitting such models is
difficult, and requires robust methods for computation and
evaluation.  We adapt a line of research on black
box variational inference methods~\citep{Ranganath:2014}, particularly
for probabilistic programming~\citep{Kucukelbir:2017,Tran:2017}, 
to be able to quickly develop and test our models.
Variational inference approximates the posterior by fitting a family
of distributions over the latent variables to be close in KL
divergence~\citep{Jordan:1999,Blei:2017}.  For simplicity, consider
the problem of fitting embeddings.  The variational
distribution is $q(\alpha_{1:V}, \rho_{1:V} ; \nu)$ and we fit the
variational parameters $\nu$ to be close in KL divergence to
$p(\alpha_{1:V}, \rho_{1:V} \g \mbx)$.  Recent innovations in
probabilistic programming and variational autoencoders
\citep{kingma} (as implemented, for example, in Google TensorFlow) also us to do this generically and
scalably, easily fitting many types of models to large
data sets.  This enables the exploration of many variants of the
models, e.g., different types of contexts, different values of $d$,
different underlying conditional distributions.


\project{Representations beyond co-occurrence statistics}
Distributed embedding representations in machine learning are almost
exclusively based on co-occurrence statistics. For instance, when
constructing embeddings for words in text, names of colors (``red,''
``blue''...) will be embedded in nearby locations simply because they
tend to be used together. How can a richer knowledge of representation
in the brain be used to inspire algorithms for processing text,
images, and audio? What are other brain-inspired features over which
co-occurrence can be evaluated?

\project{Embeddings for fMRI data}
Exponential family embeddings open
the door to developing more complex embeddings for
problems in cognitive neuroscience: embeddings for fMRI and other
types of data, embeddings with a complex notion of context, and
embeddings that are themselves a part of a larger probabilistic model,
such as where representations are shared or tied across tasks.  For
examples of some of these innovations see~\citet{Rudolph:2016b}.
The approach will be based on the incorporation of a latent variable model,
for which variational methods can be used for scalable inference, 
as described above.


\project{Time-dependent representations}
To capture dependence on time, we will build on one of the PIs earlier
work on dynamic topic models~\citep{Blei:2006d}.  Dynamic topic models
captured how the latent themes in a collection can grow and shrink and
change over time.  For example, the theme of ``technology'' in a
scientific corpus might start with words about electricity and wires
and end with words about computers and semiconductors.
Dynamic topic models were developed specifically for language.  We
will generalize this idea to capture the evolution over time of distributed
representations.  Unlike dynamic topic models, the fitted model
will capture multimodal data and the evolution of its latent characteristics.
In the exponential family
embedding framework, this amounts to placing a linear dynamic prior on
the embedding vectors or the context vectors (or both).  
