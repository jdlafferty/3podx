\def\argmin{\mathop{arg\,min}}

\msection{Memory Capacity (Objective 4) }
\label{sec:aim4}

Given the fixed number of neurons in the brain, there must be a
tradeoff between the number of memories stored and the precision with
which they are stored. However, there is evidence from human behavior
that people store a massive number of things in long-term memory with
incredible detail
(\href{http://cvcl.mit.edu/MM/download.html}{cvcl.mit.edu/MM}). How is
this possible? Recent work in deep learning has investigated
strategies for reducing memory usage, discussed briefly below.
Computational
architectures and insights could inform simple experiments to better
understand the nature of memory usage in living systems.
In the other direction, our understanding the way memories are
formed across different levels of cognition, can serve as
inspiration for computational and statistical models. In particular,
we will explore computational learning frameworks treating memory
in terms of inversion of generative processes.

\biobackground{}
Kandel's ``In Search of Memory,'' from sea slugs to humans,
neural mechanisms of habituation, etc.?

\statbackground{} Memory is a foundational theme throughout the broad
landscape computation. In information theory, the notion of memory was
probed in Shannon's landmark paper, which presaged hidden Markov
models in the discussion of finite state models of language, with a
state representing the ``residue of influence'' of the previous
letters \citep{Shannon:48}. A rich mathematical literature exists that
explores the mathematical properties of finite and infinite stochastic
processes with respect to notions of memory, including mixing
properties of chains \citep{pollard84}. Popular neural network models
for more flexibly handling long-range dependence include LSTMs and
their variants \citep{sepp97,gers00}. This latter work serves to
remind us that even simple metaphors for learning in humans, when
implemented in scalable and easily available code, can lead to methods
having great impact in contemporary applications.

Recent work in deep learning has investigated
strategies for reducing memory usage \citep{ChenXZG16}. During a
forward pass, only a subset of the nodes' activations are kept in
memory, and the others are discarded. In back-propagation, the
computation is redone at the forgotten nodes. This induces a trade-off
between time complexity and memory. The practical problem is to
determine the subset of nodes to checkpoint, given the network and a
memory budget. The tradeoff can be thought of in terms a simple pebble
game. Insights from neuroscience could inform novel algorithms for
reducing memory in artificial learning algorithms. Other
work on tradeoffs in statistical learning includes
\citep{lucic15tradeoffs}.

A new line of research is shedding light on properties of deep
learning algorithms. This may ultimately lead to better understanding
of the ability of deep networks trained with adversarial networks, or
with variational autoencoders, to generate synthetic data that appears
natural. We propose to investigate such generative models as a
computational paradigm for memory formation. 
Work over the past 3-5 years has analyzed a range nonconvex
optimization problems, including phase retrieval, compressed
sensing under sparsity constraints, low-rank matrix
completion, and randomized sensing of low rank matrices
\cite{phaselift_1,phaselift_2,phaselift_3,ZhaWanLiu15,WeiCaiCha16,ZheLaf15,phaselift_1,phaselift_2,phaselift_3,ZhaWanLiu15,WeiCaiCha16}.
Some of this work is of the nature of the results in this
paper, that the local optima are either non-existent, or
can be avoided by simple initialization schemes. Many of
the results make assumptions of random problem instances.
Very recent work has established technical results
that make progress toward showing it is possible to invert randomly constructed
deep networks. For example, networks are considered that compose multiple ReLU-transformed linear transformations,
which is a simple but often practically effective  deep learning architecture.
In particular, the results of \cite{Mixon18} make use of the machinery
of spherical harmonics and Gegengauer polynomials for establishing
inversion algorithms for certain two-layer networks.
\cite{HandV17} show that the inversion objective function for such networks, based
on empirical risk minimization, does not have spurious stationary points,
so that at any point away from an optimal solution has a descent
direction. Together with more recent results of \cite{HandV18},
this line of work begins to establish compressed sensing theory
for a rich family of generative models. 

% generative models as memory
% hand, voroninski
% optimization with backtracking memories
% risk bounds as a function of memory in density estimation

\project{Nonparametric estimation with memory constraints}
As discussed earlier, recent work has established 
sharp constrained minimax analysis of nonparametric
regression under quantization and communication constraints \citep{Zhu:18,Zhu:18b}.
It is of interest to understand how memory restrictions might affect
statistical rates of convergence. While this earlier work focuses on
normal means formulations, a more practical framework for
nonparametric estimation is local averaging using kernels. In
classical theory, the actual form of the kernel does not affect the
rate of convergence, as it enters as a constant in the bias and
variance calculations. However, the kernel can be viewed as a
simple formalization of memory---a kernel evaluation $K(x,x')$ asks
``how similar is the current stimulus $x$ to a previous stimulus $x'$
observed in my past experience?'' We will consider mathematical models for
how the kernel $K$ degrades under memory constraints that
are motivated by empirical findings in cognitive neuroscience,
and derive minimax results under such memory constraints. The
Tsybakov low-noise condition will be used to allow for
high-dimensional learning \citep{mammen1999,tsybakov2004}.



\project{Inversion of nonconvex generative models using memory landmarks}
One of the many outstanding questions that prevents an understanding
of the many practical successes of deep learning is the efficacy of
stochastic gradient descent. Many practical systems are constructed
with deep convolutional networks using ReLUs; these represent
hierarchical compression schemes. Their corresponding generative
counterparts are ``decoder networks'' which take a dense Gaussian
input and expand it to, for example, a synthetic image.  We will study
models of memory in cognition that are based on inverting
generative/decoder networks. The recent of work of \cite{HandV17} (see
also \cite{HandV18}) shows that random generative ReLU networks have a
``nice'' decoding function. In particular, if $G(z)$ is such a
network, and $A$ is a random matrix, then the empirical risk function
$ f_x(z) = \|AG(z) - Ax\|_2^2$ is remarkably
well-behaved. Specifically, under natural conditions on the scaling of
the network, the function $f(z)$ has no local optima or critical
points (outside of two neighborhoods around the optima). This opens
the door for a new line of mathematical work on compressive sensing
for generative models. We will investigate computational questions
motivated by metaphorical notions of memory, plasticity and learning
in the brain in this setting. In particular, suppose that the weight
matrices of such a network are initialized randomly; thus every point
has a descent direction. As the network is fed data, and the weight
matrices are optimized to learn to code for those data.  During this
learning phase, where the network weights are reconfigured
(plasticity), they become nonrandom, and the function $f_x(z)$ becomes
highly nonconvex. We will study a algorithmic technique where the
descent directions at intermediate points are noted and remembered (or
``memoized'') during this learning phase. As the landscape becomes
highly nonconvex, these previous descent directions are used for
searching and decoding by gradient descent, which is used to assess
future data against past experience.

